# Large Language Models

### Tokenization
- Break down sentences into smaller components
- Tokens - Words, numbers, or punctuation marks

