# Large Language Models

### LLM Evaluation - Why ?
- Reliability
- Effectiveness

### LLM Evaluation - How ?
- LLM Model Evaluation
- LLM System Evaluation
  
### LLM Model Evaluation
- TruthfulQA
- Massively Multitask Language Understanding
  
### LLM System Evaluation 
-
-

### LLM Frameworks
- LangChain - Text Summarisation, Question Answering and Chatbots
- LlamaIndex - Document Q&A, Retrieval Augmented Generation, Knowledge Agents
- Haystack - Semantic Search Engines, Information Retrieval Systems
- HuggingFace - 

### Fine-tune
- Layer-wise Fine-Tuning
- Parameter Selective Fine-tuning
- Adapter-based Fine-tuning

### Low Rank Adaptation 
- LoRA
- LoRA - Why ?
  - LLM fine-tuning
    - Compute intensive
    - Memory intensive
    - Data intensive
    - Extensive Hardware
  - Large Models - Low intrinsic dimension
    - Fewer trainable parameters - Comparable performance
- LoRA - How ?
  - Fine-tune LLM
  - Small dataset
  - Few parameetrs
  - LLM adaptors
-   

#### References

##### LLM
- [Introduction to LLM Evaluation: Navigating the Future of AI Technologies]([)](https://medium.com/codecontent/introduction-to-llm-evaluation-navigating-the-future-of-ai-technologies-e1fcd5db2e04)
- [Choosing the Right Generative AI Framework-LangChain, LlamaIndex, Haystack, or Hugging Face](https://generativeai.pub/choosing-the-right-generative-ai-framework-langchain-llamaindex-haystack-or-hugging-face-29a6b23b2ca3)

##### LoRA
- [Low Rank Adaptation (LoRA): From Intuition to Implementation to Interview Questions](https://medium.com/@AnveeNaik/low-rank-adaptation-lora-from-intuition-to-implementation-to-interview-questions-1461c6a81615)
- [Fine-Tuning Large Language Models with LORA: Demystifying Efficient Adaptation](https://medium.com/@kailash.thiyagarajan/fine-tuning-large-language-models-with-lora-demystifying-efficient-adaptation-25fa0a389075)
  
### Natural Language Processing
- NLP
- Part-of-Speech (POS) Tagging
  - Categorize each token
  - Noun, verb, or adjective
- Creating N-Grams
  - Sequences of 'N' consecutive words
  - Provide context
- Stemming
  - Reduce words to their base or root form 
- Lemmatization
       
### Tokenization
- First step in NLP pipeline
- Break down sentences into smaller components
- Tokens - Words, numbers, or punctuation marks

### Subword Tokenization
- Break down text into subwords

### Byte Pair Encoding 
- BPE

### WordPiece
-

### SentencePiece
-
